{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data Models and Contributions\n",
    "---\n",
    "**Author**: Chris Meyer, PhD; Center for Translational Data Science at the University of Chicago; July 2025\n",
    "\n",
    "**Overview**: This notebook builds synthetic data models based on a set of input data models, and then it creates several synthetic data contributions consisting of a collection of TSVs and a simple data dictionary for those TSVs. \n",
    "\n",
    "**Input**: 27 real, biomedical data models that were obtained from Gen3 data commons APIs.\n",
    "\n",
    "**Output**: _n_ synthetic data models and a collection of synthetic data contributions based on those synthetic models. The number of contributions depends on the parameters used in the synthetic data contribution script, but essentially, subsets of nodes from each synthetic data model are chosen and then headers are grouped into TSVs using an algorithm that flattens the model below each branching point. In a totally linear model, all headers and foreign keys are flattened into a single TSV. In a highly-branched model, many TSVs will be produced. A file manifest TSV is also created for each file node, which mimics real-world data contributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Pseudocode for synthetic data model generation:\n",
    "1. Obtain the real Gen3 data model schema.json files from Gen3 APIs. (Currently, fetching 27). Note: The 27 used in this demo are already retrieved and packaged with this notebook.\n",
    "2. Define run parameters:\n",
    "    1. min_add_node_rounds (int, default: 6): the minimum number of times to sample nodes from the input data models.\n",
    "    2. max_add_node_rounds (int, default: 15): the maximum number of times to sample nodes from the input data models.\n",
    "    3. n (int, default: 10000): the number of data models to generate\n",
    "    4. input_schemas (list of filenames): the list of data model schemas that will serve as the pool of data models from which to sample nodes/properties.  Note: in the 21 input, real Gen3 data models, there are on average ~26 nodes and ~17 properties per node.\n",
    "    5. min_props_perc (float, default: 0.25): the minimum percentage of optional properties to keep.\n",
    "    6. max_props_perc (float, default: 0.75): the maximum percentage of optional properties to keep.\n",
    "\n",
    "3. Read in the real Gen3 data model schema.json files listed in input_schemas.\n",
    "4. Compile a master “root node”, which in this case is based on the “project” node in Gen3. This master “project” node contains all properties from all “project” nodes across all Gen3 data models. Every Gen3 data model has a project node. It simply serves the purpose of having a root node to connect all the variable nodes to in the SDMs.\n",
    "5. Produce a master set of input nodes, excluding Gen3 system / root nodes. Note: The current node pool for 21 Gen3 data models is 237 nodes; this is one place where we could improve synthetic data model diversity, e.g., by bringing in additional (non)-Gen3 data models.\n",
    "6. Do the following _n_ number of times to generate individual SDMs.\n",
    "7. Select a random integer add_node_rounds between the min_ / max_add_node_rounds parameters, which will be the number of times to sample nodes from the master set of input nodes; defaults are 6 (min) to 15 (max) rounds to generate SDMs that have ~25-30 total nodes.\n",
    "8. For add_node_rounds (int) number of times:\n",
    "    1. From the master pool of nodes from real input models, get the set of nodes not already present in the SDM being created. \n",
    "    2. Choose a random node from that set of nodes.\n",
    "    3. Randomly select an input data model from the set of input data models that contain the randomly selected node.\n",
    "    4. Find the path from the selected node up to the root node, i.e., the set of parent nodes leading from the selected node up to the project node.\n",
    "    5. For the selected node and all other nodes in its path up to the root node that are not already in the SDM, build a simplified node definition and add it to the SDM’s list of nodes.\n",
    "        1. Simplified node definitions include the node name, description, links, and a list of properties.\n",
    "        2. The list of properties includes all properties listed as “required” in the input data model as well as all other “optional” properties, a node “id” property (e.g., “sample.id” for the sample node), and a property for each link (e.g., “subject.id” for the link from the “sample” node to the “subject” node).\n",
    "        3. Anything specific only to the Gen3 software, like system properties, is removed.\n",
    "        4. A random integer num_props is selected that falls between the number of properties in the model multiplied by min_props_perc and max_props_perc. This is the number of optional properties to include in the node’s properties list.\n",
    "        5. num_props properties are randomly selected from the original node’s list of optional properties to include in the SDM node’s properties list. By default, this results in a random selection of between 25-75% of the node’s original properties to be included in the SDM.\n",
    "10. Synonymous nodes that exist in the SDM node list are now merged into a randomly chosen synonymous node name.\n",
    "    1. First, synonymous nodes are found. These are nodes that occupy the same space in the model and serve the same function. Currently, these are the only synonyms being used for this: ['study', 'dataset', 'clinical_trial', 'collection', 'research'] and ['subject', 'patient', 'case', 'participant'].\n",
    "    2. If synonymous nodes are found in the SDM after adding nodes, a single synonym is randomly chosen (e.g., choose “dataset” if both “study” and “dataset” exist in the SDM) and the synonymous nodes are merged.\n",
    "    3. Links and properties of all nodes are updated to reflect the chosen synonym. \n",
    "11. The SDM is written to a file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "If running this notebook outside of the cloned repo, let's pull the repo to get the helper files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/uc-cdis/abdi-synthetic-data-tools.git\n",
    "%cd abdi-synthetic-data-tools/notebooks/synthetic_data_contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from sdm_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Define main variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Google Colab Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = \"/content\"  # your local home directory\n",
    "git_dir = f\"{home_dir}/abdi-synthetic-data-tools\"\n",
    "nb_dir = (\n",
    "    f\"{git_dir}/notebooks/synthetic_data_contributions\"  # notebook working directory\n",
    ")\n",
    "schema_dir = (\n",
    "    f\"{nb_dir}/simplified_gen3_schemas\"  # directory containing input data model schemas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Local Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_dir = \"/Users/user\"  # your local home directory\n",
    "# git_dir = f\"{home_dir}/Documents/GitHub/uc-cdis/abdi-synthetic-data-tools\"\n",
    "# nb_dir = (\n",
    "#     f\"{git_dir}/notebooks/synthetic_data_contributions\"  # notebook working directory\n",
    "# )\n",
    "# schema_dir = (\n",
    "#     f\"{nb_dir}/simplified_gen3_schemas\"  # directory containing input data model schemas\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Other Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: real Gen3 dms have an average of 35.25 nodes \n",
    "# min/max_add_node rounds are 6, 15 for generating close to that average number of nodes per model\n",
    "min_add_node_rounds = 6\n",
    "max_add_node_rounds = 15\n",
    "\n",
    "n=10000\n",
    "n=10\n",
    "min_props_perc = 50 # percentage of properties to keep at minimum\n",
    "max_props_perc = 100 # max percentage of properties to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dir\n",
    "date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "sdm_dir = f\"{nb_dir}/SDM/SDMs_nodes{min_add_node_rounds}-{max_add_node_rounds}_props{min_props_perc}-{max_props_perc}_{date}\"\n",
    "Path(sdm_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"{n} synthetic Data Models will be output to: \\n{sdm_dir}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to keep track of statistics and information about the synthetic data models\n",
    "sdms = {}\n",
    "sdms['max_add_node_rounds'] = max_add_node_rounds\n",
    "sdms['min_add_node_rounds'] = min_add_node_rounds\n",
    "sdms['min_props_perc'] = min_props_perc\n",
    "sdms['max_props_perc'] = max_props_perc\n",
    "sdms['date'] = date\n",
    "sdms['out_dir'] = sdm_dir\n",
    "node_counts = {} # a dictionary of all nodes across all input data models and their counts\n",
    "prop_counts = {} # a dictionary of all properties across all input data models and their counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Read in simple data model schemas\n",
    "---\n",
    "In order to be agnostic to the exact structure of the input/output data models, the following code expects input data models to be in this structure and will generate output synthetic models in this format as well, which can then be converted to other formats (graph, relational, linkML, Gen3, etc.):\n",
    "```\n",
    "{\n",
    "  \"nodes\": [\n",
    "    {\n",
    "      \"name\": \"<node_name>\",\n",
    "      \"description\": \"The description of this node.\",\n",
    "      \"links\": [<a list of node names that this node links to>],\n",
    "      \"required\": [<a list of properties that are required (vs. optional)>],\n",
    "      \"properties\": [\n",
    "        {\n",
    "          \"name\": \"<property name>\",\n",
    "          \"description\": \"The description of this property.\",\n",
    "          \"type\": \"<the data type of this property, e.g., string, integer/number, enumeration, array, etc.>\"\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a list of simple data model schemas\n",
    "#schema_dir = f\"{home_dir}/Documents/Notes/AI/AI_data_curation/input_schemas/gen3_schemas/simplified_gen3_schemas_2025-04-23\"\n",
    "schemas = sorted([f\"{schema_dir}/{f}\" for f in os.listdir(schema_dir) if f.endswith('__jsonschema_dd.json')])\n",
    "sdms['input_schemas'] = schemas # a list of input data model schema files\n",
    "dms = read_schemas(schemas) # average node count for the 20 real Gen3 dds  = 35.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If not using the example schemas provided in this demo (the 27 simplified_gen3_schemas), then uncomment lines in this cell\n",
    "# # write input data model schemas to the output dir in a subdir named \"input_schemas\" as a reference\n",
    "# input_schemas_dir = f\"{sdm_dir}/input_schemas\"\n",
    "# #!rm -r $input_schemas_dir\n",
    "# Path(input_schemas_dir).mkdir(parents=True, exist_ok=True)\n",
    "# for dm in schemas:\n",
    "#     shutil.copy(dm,input_schemas_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of node and property counts for each output synthetic data model generated \n",
    "all_nodes = get_node_set(dms,{'nodes':[]})\n",
    "node_counts = {n:0 for n in all_nodes}\n",
    "all_props = get_prop_set(dms)\n",
    "prop_counts = {p:0 for p in all_props}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Create _n_ synthetic data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n synthetic data models with the project node as the root node.\n",
    "sdms['models'] = {}\n",
    "for i in range(0,n):\n",
    "    sdms['models'][i] = {} # for tracking\n",
    "    sdm = {} # the new synthetic data model being built\n",
    "    sdm['nodes'] = []\n",
    "    proj_def = create_master_project(dms)\n",
    "    sdm['nodes'].append(proj_def)\n",
    "    # A node name is randomly sampled from the set of node names from the input Gen3 data models.\n",
    "    # This is done a random number of times to build a synthetic data model. (~6 to 15 rounds)\n",
    "    add_node_rounds = random.randint(min_add_node_rounds,max_add_node_rounds)\n",
    "    sdms['models'][i]['add_node_rounds'] = add_node_rounds # tracking\n",
    "    sdms['models'][i]['nodes'] = {} # tracking\n",
    "    for j in range(0,add_node_rounds):\n",
    "        node_set = get_node_set(dms,sdm)\n",
    "        random_node = random.sample(sorted(node_set),1)[0]\n",
    "        sdms['models'][i]['nodes'][random_node] = {} # tracking\n",
    "        node_dms = get_node_dms(random_node,dms)\n",
    "        # pick a random dm from the list of dms that contain the random node\n",
    "        random_dm_name = random.sample(list(node_dms),1)[0]\n",
    "        sdms['models'][i]['nodes'][random_node]['dm_name'] = random_dm_name\n",
    "        random_dm = node_dms[random_dm_name]\n",
    "        node_def = [n for n in random_dm['nodes'] if n['name'] == random_node][0]\n",
    "        sdm['nodes'].append(node_def)\n",
    "        #print(f\"\\t({j+1}/{add_node_rounds}) Adding node: '{random_node}' from data model: '{random_dm_name}'\")\n",
    "        # Get a random node path from the selected node up to the project node and add that path to SDM.\n",
    "        random_node_path = get_random_path(random_node,random_dm)\n",
    "        sdms['models'][i]['nodes'][random_node]['path'] = random_node_path\n",
    "        # get path nodes not already in sdm\n",
    "        parent_nodes = [n for n in random_node_path if n not in [n['name'] for n in sdm['nodes']]]\n",
    "        sdms['models'][i]['nodes'][random_node]['parent_nodes'] = parent_nodes\n",
    "        #print(f\"\\t\\tAdding parent nodes: {parent_nodes}\")\n",
    "        for parent_node in parent_nodes:\n",
    "            node_def = [n for n in random_dm['nodes'] if n['name'] == parent_node][0]\n",
    "            sdm['nodes'].append(node_def)\n",
    "            #print(f\"\\t\\tAdding node: {parent_node} from data model: '{random_dm_name}'.\")\n",
    "        sdm = fix_links(sdm) # fix links for the new nodes added to the sdm\n",
    "\n",
    "    # If there are any synonyms, pick one and merge into the chosen name\n",
    "    odm = copy.deepcopy(sdm) # troubleshooting\n",
    "    syn_lists = get_sdm_synonyms(sdm)\n",
    "    sdms['models'][i]['synonyms'] = syn_lists\n",
    "    sdm = update_sdm_synonyms(sdm,syn_lists)\n",
    "\n",
    "    # # FIX LINKS bc chose random paths (remove links that are not in the sdm)\n",
    "    # odm = copy.deepcopy(sdm) # troubleshooting\n",
    "    # sdm = fix_links(sdm)\n",
    "\n",
    "    # order SDM by submission order\n",
    "    suborder = get_submission_order(sdm)\n",
    "    sdms['models'][i]['suborder'] = suborder\n",
    "    sdm['nodes'] = sorted(sdm['nodes'], key=lambda x: suborder[x['name']])\n",
    "\n",
    "    # Randomly select optional properties for each node in the synthetic data model.\n",
    "    sdm = select_optional_properties(sdm, min_props_perc=min_props_perc, max_props_perc=max_props_perc)\n",
    "    \n",
    "    # Truncate descriptions in the synthetic data model to a specified length.\n",
    "    sdm = truncate_sdm_descriptions(sdm,desc_limit=1000,log_file=f\"{sdm_dir}/description_truncation_log.json\")\n",
    "\n",
    "    # update node_counts and prop_counts\n",
    "    sdm_nodes = list_nodes(sdm)\n",
    "    sdms['models'][i]['node_count'] = len(sdm_nodes)\n",
    "    \n",
    "    for node_def in [n for n in sdm['nodes'] if n['name']!= 'project']:\n",
    "        node = node_def['name']\n",
    "        node_counts[node]+=1\n",
    "        for prop_def in node_def['properties']:\n",
    "            node_prop = f\"{node}.{prop_def['name']}\"\n",
    "            if node_prop in prop_counts:\n",
    "                prop_counts[node_prop]+=1\n",
    "\n",
    "    # write the synthetic data model to a json file.\n",
    "    out_file = f\"{sdm_dir}/SDM_{i}.json\"\n",
    "    with open(out_file,'w') as f:\n",
    "        json.dump(sdm,f,indent=2)\n",
    "    print(f\"({i+1}/{n}) Synthetic data model: {out_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort node and prop counts by their values\n",
    "sdms['node_counts'] = sorted(node_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sdms['prop_counts'] = sorted(prop_counts.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average node count per sdm\n",
    "average_node_count = sum([sdm['node_count'] for sdm in sdms['models'].values()])/len(sdms['models']) \n",
    "sdms['average_node_count'] = average_node_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sdms dictionary to a json file\n",
    "out_file = f\"{sdm_dir}/_SDM_stats.json\"\n",
    "with open(out_file,'w') as f:\n",
    "    json.dump(sdms,f,indent=2)\n",
    "print(f\"SDM stats written to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Create Synthetic Data Contributions\n",
    "---\n",
    "The following code will take input data models (format same as detailed above) and generate a variable number of random synthetic data contributions that mimic real-world, biomedical data contributions. The purpose of generating these is to use them to train AI models to take a dump of random TSVs containing headers as data attributes as input and output a reasonable draft data model that both organizes and describes the data elements in a dataset and can be used to either start the process of data harmonization to an existing data model or create a new data model from scratch for a collection of structured, semi-structured and/or unstructured data files.\n",
    "\n",
    "Input models can be real data models like those used to generate the synthetic data models in the previous section above, or they can be synthetic data models, like those generated above. In this demo notebook, the input data models are the synthetic data models generated above. \n",
    "\n",
    "### Pseudocode for synthetic data contribution generation:\n",
    "1. Identify leaf nodes and file nodes in the input data model.\n",
    "2. Get the paths from each leaf node up to the root node (`project`). There may be overlap in the nodes in the paths, but by definition, no leaf node's path will be entirely contained in another path.\n",
    "3. Randomly sample leaf node paths without replacement to get groups of nodes/paths to use for synthetic data contributions.\n",
    "4. For each group of leaf node paths:\n",
    "    1. Get the path from each leaf node to the root node.\n",
    "    2. Find the file nodes in keys and values of leaf_paths.\n",
    "    3. Merge leaf_paths and file_paths.\n",
    "    4. Count the number of times each node occurs in a path (\"path count\") and sort the counts by value; this informs how to merge node headers into TSVs.\n",
    "    5. Create a dictionary to save lists of properties. These will later become TSV file headers named by the dict keys (node names).\n",
    "    6. For each leaf node path:\n",
    "        1. Get the path count for each node in the path.\n",
    "        2. Starting with the lowest path count (most terminal nodes in the path), for each path count value in the group of nodes:\n",
    "            1. Find the file nodes and add the file node name as key to the dictionary of properties with its list of properties as value.\n",
    "            2. For all other nodes:\n",
    "                1. Find the base node in the path (the most terminal node). This will be the TSV name / key in dict of property lists.\n",
    "                2. Merge the properties of each node with the same path count into a list, including foreign keys of any file nodes.\n",
    "                3. Add the base node as key and the list of properties as a value to the dictionary of property lists.\n",
    "    7. For each key/list of properties, write the properties to a TSV file as headers and name it with the key. For non-file nodes, the TSV file with be named \"`node_name`_metadata.tsv\", and for file nodes the TSV is named \"`node_name`_file_manifest.tsv\".\n",
    "    8. Take the subset of nodes in the node group and create a simple data model/dictioanry that is a subset of the input data model containing only nodes / properties in the TSVs.\n",
    "5. Validate the TSVs and simplified data models:\n",
    "    1. Check that every header in each TSV is in the accompanying data model\n",
    "    2. Check that all properties in simplified_dd.json files are in a TSV.\n",
    "6. Write out run parameters and some statistics on the generated synthetic data contributions.\n",
    "7. Generate a plot of the property counts per TSV, which is important to know for using the synthetic data in AI training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for creating the synthetic data contributions\n",
    "from sdc_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters for the synthetic data contributions\n",
    "date = datetime.now().strftime(\"%Y%m%d\")\n",
    "node_limit, node_min = 2, 2 # max/min number of nodes to group for creating SDCs\n",
    "parent_prefix = False # don't prefix property names in headers with the node they come from\n",
    "keep_min = 25 # percentage of properties to keep; default is 25\n",
    "keep_max = 75 # percentage of properties to keep; default is 75\n",
    "hard_limit = 20 # max property count per node; setting lower hard_limit for the handful of nodes with over 200 properties\n",
    "desc_limit = 1000 # default is 2048\n",
    "exclude_props = ['associated_ids', 'authz', 'callset', 'case_ids', 'case_submitter_id', 'created_datetime', 'error_type', 'file_state', 'ga4gh_drs_uri', 'id', 'project_id', 'state', 'state_comment', 'subject_ids', 'submitter_id', 'token_record_id', 'type', 'updated_datetime']\n",
    "print(f\"\\tNode max limit for grouping (node_limit): {node_limit}\")\n",
    "print(f\"\\tNode minimum for grouping (node_min): {node_min}\")\n",
    "print(f\"\\tMinimum percentage of properties to keep (keep_min): {keep_min}\")\n",
    "print(f\"\\tMaximum percentage of properties to keep (keep_max): {keep_max}\")\n",
    "print(f\"\\tHard limit for number of properties per node to keep (hard_limit): {hard_limit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdc_name = f'sdc_v3_nmax{node_limit}_nmin{node_min}_pmax{keep_max}_pmin{keep_min}_limit{hard_limit}_dmax{desc_limit}_{date}' # name of the SDC collection to be produced\n",
    "sdc_dir = f\"{nb_dir}/SDC/{sdc_name}\" # directory where the synthetic data contributions will be output\n",
    "\n",
    "## Synthetic Data Models\n",
    "sdm_files = sorted(glob.glob('{}/SDM_*.json'.format(sdm_dir)))\n",
    "sdm_files = sdm_files[0:10] # subset for testing\n",
    "schema_files = sorted(sdm_files,key=lambda x: int(x.split('SDM_')[-1].split('.json')[0])) # sort the synthetic_schema files by the number in the filename\n",
    "\n",
    "## Uncomment the lines here if you want to generate SDCs based on the 27 real data models used as input in the section above instead of using the synthetic data models generated as output in that section. \n",
    "# ## Simplified Gen3 Data Models\n",
    "# schema_files = glob.glob(f\"{schema_dir}/*.json\")\n",
    "\n",
    "print(f\"\\nRunning synthetic data contribution generation for {len(schema_files)} data models with the above parameters found in:\\n{sdm_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the synthetic data contributions\n",
    "for i in range(0,len(schema_files)): # schema_file = schema_files[0]\n",
    "    #schema_file = f\"{home_dir}/Documents/Notes/AI/AI_data_curation/randomize_dd/input_schemas/gen3.biodatacatalyst.nhlbi.nih.gov_schema.json\"\n",
    "    schema_file = schema_files[i]\n",
    "    dm = read_json(schema_file)\n",
    "\n",
    "    schema_name = schema_file.split('/')[-1].split('.json',)[0]\n",
    "    print(f\"({i}/{len(schema_files)}) {schema_name}: {schema_file}\")\n",
    "    out_dir=f'{sdc_dir}/{schema_name}_{date}_tsvs'\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ## 1. Identify leaf nodes in simplified data model and get paths from each node to root node. There may be signficant overlap in the nodes in the paths, but by definition, no leaf node's path will be entirely contained in another path.\n",
    "    leaf_nodes = get_leaf_nodes(dm)\n",
    "    file_nodes = get_file_nodes(dm, use_file_name=True) # nodes that have a file_name property and will need a file manifest TSV\n",
    "\n",
    "    all_leaf_paths = get_node_paths(dm,nodes=leaf_nodes) ## Get all leaf node paths\n",
    "    all_file_paths = get_node_paths(dm,nodes=file_nodes) ## Get all file node paths\n",
    "    all_paths = {**all_leaf_paths,**all_file_paths}\n",
    "    all_counts = {}\n",
    "    for node in [n['name'] for n in dm['nodes']]:\n",
    "        pcount = len([path for path in list(all_paths.values()) if node in path])\n",
    "        all_counts[node] = pcount\n",
    "    all_counts = dict(sorted(all_counts.items(), key=lambda item: item[1],reverse=True)) #sort pcounts by value\n",
    "    max_count = max(list(all_counts.values()))\n",
    "\n",
    "    ## 2. Randomly sample paths without replacement to get a set of paths to use for synthetic data contributions\n",
    "    model_headers = {}\n",
    "    node_groups = randomly_group_nodes(nodes=leaf_nodes,node_limit=node_limit,node_min=node_min)\n",
    "    #display(node_groups) # each node_group becomes a single training example\n",
    "    write_json(node_groups,f\"{out_dir}/{schema_name}_node_groups.json\") # write node_groups to a JSON file\n",
    "\n",
    "    for j in range(0,len(node_groups)): # node_group = node_groups[0]\n",
    "        node_group = node_groups[j]\n",
    "        ## Create output directory\n",
    "        groupname = '.'.join(node_group)\n",
    "        groupdir = f\"{out_dir}/{groupname}\"\n",
    "        Path(groupdir).mkdir(parents=True, exist_ok=True)\n",
    "        ## Get the path from each leaf node to the root node\n",
    "        leaf_paths = {node:all_paths[node] for node in node_group}\n",
    "        ## Find the file nodes in keys and values of leaf_paths\n",
    "        group_files = list(set([n for n in list(leaf_paths.keys()) + list(set([node for path in list(leaf_paths.values()) for node in path])) if n in file_nodes]))\n",
    "        file_paths = {node:all_paths[node] for node in group_files}\n",
    "        group_paths = {**leaf_paths,**file_paths}\n",
    "        write_json(group_paths,f\"{groupdir}/{groupname}_paths.json\") # write group_paths to a JSON file in the group dir\n",
    "        group_nodes = order_nodes(list(set(list(group_paths.keys()) + list(set([node for path in list(group_paths.values()) for node in path])))),dm) # all nodes in the group\n",
    "        ## merge leaf_paths and file_paths\n",
    "        pcounts = {}\n",
    "        for node in group_nodes:\n",
    "            pcount = len([path for path in list(group_paths.values()) if node in path])\n",
    "            pcounts[node] = pcount\n",
    "        #sort pcounts by value\n",
    "        pcounts = dict(sorted(pcounts.items(), key=lambda item: item[1],reverse=True))\n",
    "        max_pcount = max(list(pcounts.values()))\n",
    "        gheaders = {} # group headers\n",
    "        all_headers = {} # all headers per node in the group that will be in a TSV\n",
    "\n",
    "        for l in range(0,len(leaf_paths)): # leaf_node = list(leaf_paths.keys())[-1]\n",
    "            leaf_node = list(leaf_paths)[l]\n",
    "            path_counts = {n:pcounts[n] for n in leaf_paths[leaf_node]}\n",
    "            path_counts[leaf_node] = 1\n",
    "\n",
    "            for k in sorted(list(set(path_counts.values()))): # k = sorted(list(set(path_counts.values())))[0]\n",
    "                merge_nodes = [n for n in path_counts if path_counts[n] == k]\n",
    "                suborder = {n:get_submission_order(dm)[n] for n in merge_nodes}\n",
    "\n",
    "                # add files to all_headers and gheaders\n",
    "                merge_files = [n for n in merge_nodes if n in file_nodes]\n",
    "                for file_node in merge_files: # file_node = merge_files[0]\n",
    "                    if file_node not in all_headers:\n",
    "                        rfile_headers = randomize_headers(file_node,dm,keep_min,keep_max,hard_limit)\n",
    "                        ofile_headers = organize_headers(rfile_headers,file_node,dm)\n",
    "                        all_headers[file_node] = ofile_headers\n",
    "                        #print(f\"Adding file_node '{file_node}' headers to all_headers: {ofile_headers}\")\n",
    "                    else:\n",
    "                        ofile_headers = all_headers[file_node]\n",
    "                        #print(f\"Already seen file_node '{file_node}' headers: {ofile_headers}\")\n",
    "                    gheaders[file_node] = ofile_headers\n",
    "                    #print(f\"Adding file_node '{file_node}' headers to gheaders: {ofile_headers}\")\n",
    "\n",
    "                # find non-file nodes (\"metadata\" nodes) and add to all_headers and gheaders\n",
    "                merge_meta = [n for n in merge_nodes if n not in merge_files]\n",
    "                meta_order = {n:get_submission_order(dm)[n] for n in merge_meta}\n",
    "                lowest_meta = [n for n in meta_order if meta_order[n] == max(list(meta_order.values()))] # the base node(s)\n",
    "                other_meta = [n for n in merge_meta if n not in lowest_meta] # every other node in the path besides base/lowest\n",
    "                #print(f\"i: {i},\\tmerge_nodes: {merge_nodes}, \\n\\tmerge_files: {merge_files}, \\n\\tmerge_meta: {merge_meta}, \\n\\tlowest_meta: {lowest_meta}, \\n\\tother_meta: {other_meta}\\n\") # troubleshooting\n",
    "\n",
    "                if len(lowest_meta) == 1:\n",
    "                    base_node = lowest_meta[0]\n",
    "                    #print(f\"\\tBase node: '{base_node}', other_meta nodes: {other_meta}\")\n",
    "                    if base_node not in all_headers:\n",
    "                        rbase_headers = randomize_headers(base_node,dm,keep_min,keep_max,hard_limit)\n",
    "                        obase_headers = organize_headers(rbase_headers,base_node,dm)\n",
    "                        all_headers[base_node] = obase_headers\n",
    "                        #print(f\"\\tAdding Base node '{base_node}' headers to all_headers: {obase_headers}\")\n",
    "                    else:\n",
    "                        obase_headers = all_headers[base_node]\n",
    "                        #print(f\"\\tAlready seen base_node '{base_node}' headers: {all_headers[base_node]}\")\n",
    "                    base_gheaders = copy.deepcopy(obase_headers)\n",
    "                    for other_node in other_meta: # other_node = other_meta[0]\n",
    "                        if other_node not in all_headers:\n",
    "                            rother_headers = randomize_headers(other_node,dm,keep_min,keep_max,hard_limit)\n",
    "                            oother_headers = organize_headers(rother_headers,other_node,dm)\n",
    "                            all_headers[other_node] = oother_headers # add randomized, organized headers to all_headers for sdd\n",
    "                            #print(f\"\\tAdding other_meta node '{other_node}' headers to all_headers: {oother_headers}\")\n",
    "                        else:\n",
    "                            oother_headers = all_headers[other_node]\n",
    "                            #print(f\"\\tAlready seen other_meta node '{other_node}' headers: {oother_headers}\")\n",
    "                        base_gheaders += [h for h in oother_headers if h != f'{other_node}.id'] # add the flattened node headers to base_node's randomized headers\n",
    "                        #print(f\"\\tAdding other_meta node '{other_node}' headers {oother_headers} to base_node '{base_node}' gheaders: {base_gheaders}\")\n",
    "                    obase_gheaders = organize_headers(base_gheaders,base_node,dm) # add flattened headers to gheaders for TSV creation\n",
    "                    gheaders[base_node] = obase_gheaders\n",
    "                    #print(f\"\\tAdding gheaders for base_node '{base_node}': {obase_gheaders}\")\n",
    "                    # display(gheaders)\n",
    "                    # display(all_headers)\n",
    "                    #display(gheaders == all_headers)\n",
    "                else: # there are multiple nodes in single path with the same submission order\n",
    "                    for node in lowest_meta: # node = lowest_meta[0]\n",
    "                        if node not in all_headers:\n",
    "                            random_headers = randomize_headers(node,dm,keep_min,keep_max,hard_limit)\n",
    "                            all_headers[node] = organize_headers(random_headers,node,dm)\n",
    "                            #print(f\"Adding multi-lowest_meta '{node}' headers to all_headers: {all_headers}\")\n",
    "                        else:\n",
    "                            random_headers = all_headers[node]\n",
    "                            #print(f\"Already seen multi-lowest_meta '{node}' headers: {random_headers}\")\n",
    "                        parents = get_node_parents(node,dm)\n",
    "                        for parent in [p for p in parents if p in other_meta]:\n",
    "                            if parent not in all_headers:\n",
    "                                rheaders = randomize_headers(parent,dm,keep_min,keep_max,hard_limit)\n",
    "                                oheaders = organize_headers(rheaders,parent,dm)\n",
    "                                all_headers[parent] = oheaders\n",
    "                                #print(f\"Adding multi-parent '{parent}' headers to all_headers: {all_headers}\")\n",
    "                            else:\n",
    "                                rheaders = all_headers[parent]\n",
    "                                #print(f\"Already seen multi-parent '{parent}' headers: {rheaders}\")\n",
    "                            random_headers += [h for h in rheaders if h != f'{parent}.id']\n",
    "                            other_meta.remove(parent)\n",
    "                        oheaders = organize_headers(list(set(random_headers)),node,dm)\n",
    "                        gheaders[node] = oheaders\n",
    "                        #print(f\"Adding multi-lowest_meta '{node}' headers to gheaders: {oheaders}\")\n",
    "                    for node in other_meta: # catch any nodes left over\n",
    "                        #print(f\"Leftover node: {node}\")\n",
    "                        if node not in all_headers:\n",
    "                            random_headers = randomize_headers(node,dm,keep_min,keep_max,hard_limit)\n",
    "                            oheaders = organize_headers(random_headers,node,dm)\n",
    "                            all_headers[node] = oheaders\n",
    "                            #print(f\"Adding leftover '{node}' headers to all_headers: {all_headers}\")\n",
    "                        else:\n",
    "                            oheaders = all_headers[node]\n",
    "                            #print(f\"Already seen leftover '{node}' headers: {oheaders}\")\n",
    "                        gheaders[node] = oheaders\n",
    "                        #print(f\"Adding leftover '{node}' headers to gheaders: {gheaders}\")\n",
    "\n",
    "        gheaders = {k: gheaders[k] for k in sorted(gheaders, key=lambda x: get_submission_order(dm)[x])}\n",
    "        all_headers = {k: all_headers[k] for k in sorted(all_headers, key=lambda x: get_submission_order(dm)[x])}\n",
    "        validate_headers(gheaders,all_headers) # Check that each header in gheaders is in at least one node's all_headers\n",
    "\n",
    "        # write TSVs for each node in gheaders\n",
    "        write_headers_to_files(headers=gheaders,dm=dm,out_dir=groupdir)\n",
    "\n",
    "        # write the node_group data model schema to a json file\n",
    "        write_node_group_schema(all_headers,dm,schema_name,groupname,groupdir)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcs = glob.glob(f\"{sdc_dir}/**/*__jsonschema_dd.json\", recursive=True)\n",
    "print(f\"Generated {len(sdcs)} synthetic data contributions from the {len(schema_files)} input data models, output here:\\n{sdc_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALIDATION SCRIPT\n",
    "## validate TSVs and simplified_dd.json files contain the exact same properties\n",
    "## For each subdirectory in out_dir, open each TSV, read in the properties and then check that each is in the accompanying simplified_dd json\n",
    "# import os, json, glob, sys\n",
    "# from datetime import datetime\n",
    "# date = datetime.now().strftime(\"%Y%m%d\")\n",
    "# node_limit, node_min = 2, 1\n",
    "# min_remove_perc = 10 # default is 25\n",
    "# max_remove_perc = 50 # default is 75\n",
    "# hard_limit = 20 # setting lower hard_limit for the handful of nodes with over 200 properties\n",
    "# desc_limit = 1000 # default is 2048\n",
    "# sd_dir = f\"{home_dir}/Documents/Notes/AI/AI_data_curation/synthetic_contributions\"\n",
    "# sdc_dir = f'{sd_dir}/sdc_nmax{node_limit}_nmin{node_min}_rmax{max_remove_perc}_rmin{min_remove_perc}_pmax{hard_limit}_dmax{desc_limit}_{date}'\n",
    "#sdc_dir = f\"{home_dir}/Documents/Notes/AI/AI_data_curation/synthetic_contributions/sdc_nmax2_nmin1_rmax50_rmin10_pmax40_dmax1000_20250206\"\n",
    "\n",
    "dm_dirs = os.listdir(sdc_dir) # 10k dirs\n",
    "dm_dirs = [d for d in dm_dirs if os.path.isdir(f\"{sdc_dir}/{d}\")]\n",
    "#dm_dirs = sorted(dm_dirs,key=lambda x: int(x.split('synthetic_dd_')[-1].split('_')[0])) # sort the synthetic_schema files by the number in the filename\n",
    "# check that each dm_dir is a directory\n",
    "header_lengths = []\n",
    "for i in range(0,len(dm_dirs)):\n",
    "    dm_dir = dm_dirs[i]\n",
    "    print(f\"({i}/{len(dm_dirs)}) Checking: {dm_dir}\")\n",
    "    example_dirs = os.listdir(f\"{sdc_dir}/{dm_dir}\")\n",
    "    example_dirs = [f for f in example_dirs if os.path.isdir(f\"{sdc_dir}/{dm_dir}/{f}\")]\n",
    "    for example_dir in example_dirs: # example_dir = example_dirs[0]\n",
    "        tsvs = glob.glob(f\"{sdc_dir}/{dm_dir}/{example_dir}/*.tsv\")\n",
    "        dm_file = glob.glob(f\"{sdc_dir}/{dm_dir}/{example_dir}/*__jsonschema_dd.json\")[0]\n",
    "        with open(dm_file, 'r') as f:\n",
    "            dm = json.load(f)\n",
    "        # check that every header in each TSV is in the accompanying data model\n",
    "        for tsv in [t for t in tsvs if 'project_metadata.tsv' not in t]:\n",
    "            with open(tsv) as f:\n",
    "                headers = f.readline().strip().split('\\t')\n",
    "                header_lengths.append(len(headers))\n",
    "                for header in headers:\n",
    "                    found = False\n",
    "                    for node_def in dm['nodes']:\n",
    "                        if header in [p['name'] for p in node_def['properties']]:\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        sys.exit(f\"\\n\\n\\nHeader '{header}' in '{tsv}' not found in {dm_file}!\")\n",
    "\n",
    "        ## Now do the opposite, check that all properties in simplified_dd.json files are in a TSV\n",
    "        \"\"\" 'case_id' in sdd not found in any TSVs! \n",
    "            For this, check if node is in gheaders, if not, don't worry about {node}_id being in TSVs\n",
    "        \"\"\"\n",
    "        for node_def in dm['nodes']:\n",
    "            meta_nodes = [i.split('/')[-1].split('_metadata.tsv')[0] for i in tsvs if '_metadata.tsv' in i]\n",
    "            file_nodes = [i.split('/')[-1].split('_file_manifest.tsv')[0] for i in tsvs if '_file_manifest.tsv' in i]\n",
    "            tsv_nodes = meta_nodes + file_nodes\n",
    "            for prop in [p['name'] for p in node_def['properties']]:\n",
    "                found = False\n",
    "                for tsv in tsvs:\n",
    "                    with open(tsv) as f:\n",
    "                        headers = f.readline().strip().split('\\t')\n",
    "                        if prop in headers:\n",
    "                            found = True\n",
    "                            break\n",
    "                if not found and prop != \"project_id\" and node in tsv_nodes:\n",
    "                    sys.exit(f\"\\n\\n\\nProperty '{prop}' in '{node}' of '{dm_file}' not found in any TSVs!\")\n",
    "\n",
    "\n",
    "print(f\"Average header length: {sum(header_lengths)/len(header_lengths)}\")\n",
    "print(f\"Median header length: {sorted(header_lengths)[len(header_lengths)//2]}\")\n",
    "print(f\"Max header length: {max(header_lengths)}\")\n",
    "print(f\"Min header length: {min(header_lengths)}\")\n",
    "print(f\"Number of TSVs with header lengths greater than 100: {len([h for h in header_lengths if h > 100])} out of {len(header_lengths)}\")\n",
    "print(f\"Number of TSVs with header lengths greater than 200: {len([h for h in header_lengths if h > 200])} out of {len(header_lengths)}\")\n",
    "\n",
    "# # # get the most common header lengths and show the top 10 most common header lengths\n",
    "# top_lengths = dict(sorted({i: len([h for h in header_lengths if h == i]) for i in set(header_lengths)}.items(), key=lambda item: item[1],reverse=True))\n",
    "# for i in list(top_lengths.keys())[0:10]:\n",
    "#     print(i, top_lengths[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORTING\n",
    "# recursively search through sdc_dir for all jsonschema_dd.json files and save number to a variable\n",
    "#total_sdcs = os.system(f\"find {sdc_dir} -type f -name \\\"*_jsonschema_dd.json\\\" | wc -l\")\n",
    "total_sdcs = os.popen(f\"find {sdc_dir} -type f -name \\\"*_jsonschema_dd.json\\\" | wc -l\").read().strip()\n",
    "print(f\"Total number of synthetic data contributions in {sdc_name}: {total_sdcs}\")\n",
    "\n",
    "# get the total disk size of sdc_dir\n",
    "total_size = os.popen(f\"du -sh {sdc_dir}\").read().strip()\n",
    "total_size = total_size.split('\\t')[0]\n",
    "\n",
    "# write parameters to a log_file in sdc_dir\n",
    "log_file = f\"{sdc_dir}/{sdc_name}_parameters_log.txt\"\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(f\"Synthetic Data Contribution Parameters\\n\")\n",
    "    f.write(f\"Number of input data models: {len(schema_files)}\\n\")\n",
    "    f.write(f\"Node max limit for grouping (node_limit): {node_limit}\\n\")\n",
    "    f.write(f\"Node minimum for grouping (node_min): {node_min}\\n\")\n",
    "    f.write(f\"Minimum percentage of properties (keep_min): {keep_min}\\n\")\n",
    "    f.write(f\"Maximum percentage of properties (keep_max): {keep_max}\\n\")\n",
    "    f.write(f\"Hard limit for number of properties per node to keep (hard_limit): {hard_limit}\\n\")\n",
    "    f.write(f\"Description character limit (desc_limit): {desc_limit}\\n\")\n",
    "\n",
    "    f.write(f\"\\n\\nSynthetic Data Contribution Stats\\n\")\n",
    "    f.write(f\"Total number of synthetic data contribution examples: {total_sdcs}\\n\")\n",
    "    f.write(f\"Total number of TSVs: {len(header_lengths)}\\n\")\n",
    "    f.write(f\"Total disk size of synthetic data contributions: {total_size}\\n\")\n",
    "    f.write(f\"Average header length: {sum(header_lengths)/len(header_lengths)}\\n\")\n",
    "    f.write(f\"Median header length: {sorted(header_lengths)[len(header_lengths)//2]}\\n\")\n",
    "    f.write(f\"Max header length: {max(header_lengths)}\\n\")\n",
    "    f.write(f\"Min header length: {min(header_lengths)}\\n\")\n",
    "    f.write(f\"Number of TSVs with header lengths greater than 100: {len([h for h in header_lengths if h > 100])} out of {len(header_lengths)}\\n\")\n",
    "    f.write(f\"Number of TSVs with header lengths greater than 200: {len([h for h in header_lengths if h > 200])} out of {len(header_lengths)}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot a bell curve of header lengths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.hist(header_lengths, bins=100)\n",
    "plt.xlabel('Header Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f\"{sdc_dir.split('/')[-1]}\")\n",
    "# wrap the title so it doesn't get cut off\n",
    "plt.tight_layout()\n",
    "# label the histogram bins with the top 5 most common header length bins\n",
    "# top_5 = sorted(bin_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "# for i in range(0,5):\n",
    "#     plt.text(top_5[i][0], top_5[i][1], str(top_5[i][1]), color='red')\n",
    "# save plot to file in sdc_dir\n",
    "plt.savefig(f\"{sdc_dir}/{sdc_name}_header_lengths.png\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
